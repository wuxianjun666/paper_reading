# CenterNet: Keypoint Triplets for Object Detection

用于物体检测的关键点三要素

## Abstract 

In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of **an additional assessment inside cropped regions.** This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, **detects each object as a triplet, rather than a pair of keypoints**, which improves both precision and recall. Accordingly, **we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions.** On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/ Duankaiwen/CenterNet.

在物体检测中，基于关键点的方法经常遇到大量不正确的物体边界框的缺点，可以说是由于缺乏对裁剪区域内的额外评估。本文提出了一个有效的解决方案，以最小的成本探索各个裁剪区域内的视觉模式。我们的框架建立在一个有代表性的基于关键点的单阶段检测器上，名为CornerNet。我们的方法被命名为CenterNet，将每个物体作为一个三联体，而不是一对关键点来检测，这提高了精确度和召回率。相应地，我们设计了两个定制的模块，级联角集合和中心集合，它们丰富了左上角和右下角收集的信息，并提供了更多来自中心区域的可识别的信息。在MS-COCO数据集上，CenterNet实现了47.0%的AP，比所有现有的单阶段检测器至少要好4.9%。此外，由于推理速度比排名靠前的两阶段检测器快，CenterNet表现出了与这些检测器相当的性能。代码见https://github.com/ Duankaiwen/CenterNet。

## 1. Introduction 

  Object detection has been significantly improved with the help of deep learning, especially convolutional neural networks [12] (CNNs). In the current era, one of the most popular flowcharts, the anchor-based flowchart [11, 13, 28, 32, 34], places a set of rectangles with predefined sizes (anchors) on an image and regresses the anchors to the desired place with the help of ground-truth objects. These approaches often require a large number of anchors to ensure a sufficiently high IoU (intersection over union) rate with the ground-truth objects, and the size and aspect ratio of each anchor must be manually set. In addition, anchors and the convolutional features are usually misaligned, which is not conducive to the bounding box classification task.

   To overcome the drawbacks of anchor-based approaches, a keypoint-based object detection pipeline named CornerNet [21] was proposed. This pipeline represents each object using a pair of corner keypoints, which bypasses the need for anchor boxes and achieves state-of-the-art one-stage object detection accuracy. Nevertheless, the performance of CornerNet is still restricted by its relatively weak ability to refer to the global information of an object. That is, because each object is constructed by a pair of corners, the algorithm sensitively detects the boundaries of objects without being aware of which pairs of keypoints that should be grouped into objects. Consequently, as shown in Figure 1, CornerNet often generates incorrect bounding boxes, most of which could be easily filtered out with some complementary information, e.g., the aspect ratio. 

  To address this issue, we equip CornerNet with the ability to perceive the visual patterns within each proposed region, enabling it to identify the correctness of each bounding box by itself. In this paper, we present a low-cost yet effective solution named CenterNet, which explores the central part of a proposal, i.e., the region that is close to the geometric center of a box, with one extra keypoint. We intuit that if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in the central region of the bounding box will be predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region. The idea, as shown in Figure 1, is to use a triplet, instead of a pair, of keypoints to represent each object. 

  Accordingly, to improve the detection of center keypoints and corners, we propose two strategies to enrich center and corner information, respectively. The first strategy is center pooling, which is used in the branch for predicting center keypoints. Center pooling helps the center keypoints obtain more recognizable visual patterns within objects, which makes it easier to perceive the central part of a proposal. We achieve this by obtaining the maximum summed response in both the horizontal and vertical directions of the center keypoint on a feature map for center keypoint prediction. The second strategy is cascade corner pooling, which equips the original corner pooling module [21] with the ability to perceive internal information. We achieve this by obtaining the maximum summed response in both the boundary and internal directions of objects on a feature map for corner prediction. Empirically, we verify this two-directional pooling method is more stable, i.e., more robust to feature-level noises, which would contribute to the improvement of both precision and recall. 

  We evaluate the proposed CenterNet on the MS-COCO dataset [26], one of the most popular benchmarks for largescale object detection. CenterNet, which incorporates both center pooling and cascade corner pooling, reports an AP of 47.0% on the test-dev set, outperforming all existing one-stage detectors by a large margin. With an average inference time of 270ms using a 52-layer hourglass backbone [30] per image and 340ms using a 104-layer hourglass backbone [30] per image, CenterNet is quite efficient yet closely matches the state-of-the-art performance of the other twostage detectors.

## 2. Related Work 

  Object detection involves locating and classifying objects. In the deep learning era, powered by deep convolutional neural networks, object detection approaches can be roughly categorized into two main types of pipelines, namely, two-stage approaches and one-stage approaches. 

  Two-stage approaches divide the object detection task into two stages: extract RoIs (Region of Interesting) and then classify and regress the RoIs.

  R-CNN [12] uses a selective search method [45] to locate RoIs in the input images and uses a DCN-based region-wise classifier to classify the RoIs independently. SPP-Net [14] and Fast-RCNN [11] improve R-CNNs by extracting RoIs from the feature maps. Faster-RCNN [34] is allowed to be trained end to end by introducing RPN (region proposal network). RPN can generate RoIs by regressing the anchor boxes. Later, the anchor boxes are widely used in the object detection task. Mask-RCNN [13] adds a mask prediction branch on Faster-RCNN and can thereby detect objects and predict their masks at the same time. R-FCN [6] replaces fully connected layers with position-sensitive score maps to improve the detection of objects. Cascade R-CNN [4] addresses the problem of overfitting at training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds. keypoint-based object detection approaches [43, 29, 50, 49] are proposed to avoid the disadvantages of the use of anchor boxes and bounding box regression. Other meaningful works are proposed for different problems in object detection, e.g., a [52, 22] focus on the architecture design, a [1, 10, 37, 47] focus on the contextual relationship, and a [23, 3] focus on the multi-scale unification.

   One-stage approaches remove the RoI extraction process and directly classify and regress the candidate anchor boxes. 

  YOLO [32] uses fewer anchor boxes than other approaches (divide the input image into an S × S grid) to perform regression and classification. YOLOv2 [33] improves the performance by using more anchor boxes and a new bounding box regression method. SSD [28] places anchor boxes densely over an input image and uses features from different convolutional layers to regress and classify the anchor boxes. DSSD [9] introduces a deconvolution module into SSD to combine low- and high-level features. While R-SSD [18] uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features. RON [20] proposes a reverse connection and an objectness prior to extract multiscale features effectively. RefineDet [48] refines the locations and sizes of the anchor boxes twice, exploiting the merits of both one-stage and two-stage approaches. CornerNet [21] is another keypoint-based approach that directly detects an object using a pair of corners. Although CornerNet achieves high performance, it still has room for improvement.

## 3. Our Approach 

### 3.1. Baseline and Motivation 

  This paper uses CornerNet [21] as the baseline. For detecting corners, CornerNet produces two heatmaps: a heatmap of the top-left corners and a heatmap of the bottom-right corners. The heatmaps represent the locations of keypoints of different categories and assign a confidence score to each keypoint. In addition, CornerNet also predicts the embedding and a group of offsets for each corner. The embeddings are used to identify whether two corners are from the same object. The offsets learn to remap the corners from the heatmaps to the input image. To generate object bounding boxes, top-k left-top corners and bottom-right corners are selected from the heatmaps according to their scores. Then, the distance of the embedding vectors of a pair of corners is calculated to determine if the paired corners belong to the same object. An object bounding box is generated if the distance is less than a threshold. The bounding box is assigned a confidence score equal to the average scores of the corner pair.

  In Table 1, we provide a detailed analysis of CornerNet. We calculate the FD1 (false discovery) rate of CornerNet on the MS-COCO validation dataset, defined as the proportion of incorrect bounding boxes. The quantitative results demonstrate that the incorrect bounding boxes account for a large proportion of all bounding boxes even at low IoU thresholds, e.g., CornerNet obtains a 32.7% FD rate at IoU = 0.05. This means, 32.7 out of every 100 object bounding boxes have an IoU lower than 0.05 with the ground-truth. The FD rate of the small incorrect bounding boxes, with a value of 60.3%, is even higher, than that of larger bounding boxes. One of the possible reasons for this result is that CornerNet cannot assess the regions inside the bounding boxes. One potential method to make CornerNet [21] perceive the visual patterns in bounding boxes is to adapt CornerNet into a two-stage detector, which uses the RoI pooling [11] to assess the visual patterns in bounding boxes. However, such a paradigm is known to be computationally expensive.

  In this paper, we propose a highly efficient alternative called CenterNet to explore the visual patterns within each bounding box. For object detection, our approach uses a triplet, rather than a pair, of keypoints. By doing so, our approach still keeps a one-stage detector, but partially inherits the functionality of RoI pooling. Our approach only considers the center information, and the cost is minimal. In addition, we further introduce the visual patterns within objects into the keypoint detection process by using center pooling and cascade corner pooling.

### 3.2. Object Detection as Keypoint Triplets

The overall network architecture is shown in Figure 2. We represent each object using a center keypoint and a pair of corners. Specifically, we embed a heatmap for the center keypoints on the basis of CornerNet and predict the offsets of the center keypoints. Then, we use the method proposed in CornerNet [21] to generate top-k bounding boxes. However, to effectively filter out incorrect bounding boxes, we leverage the detected center keypoints and conduct the following procedure: (1) select top-k center keypoints according to their scores; (2) use the corresponding offsets to remap these center keypoints to the input image; (3) define a central region for each bounding box and check whether the central region contains center keypoints. Note that the class labels of the checked center keypoints should be the same as the class label that of the bounding box; (4) if a center keypoint is detected in the central region, we preserve the bounding box. The score of the bounding box is replaced by the average scores of the triple points, i.e., the top-left corner, the bottom-right corner, and the center keypoint. If there are no center keypoints detected in the central region, the bounding box will be removed.

  The size of the central region in the bounding box affects the detection results. For example, small central regions lead to a low recall rate for small bounding boxes, while large central regions lead to a low precision for large bounding boxes. Therefore, we propose a scale-aware central region to adaptively fit the size of bounding boxes. The scale-aware central region tends to generate a relatively large central region for a small bounding box and a relatively small central region for a large bounding box. Let tl_x and tl_y denote the coordinates of the top-left corner of i and br_x and br_y denote the coordinates of the bottom-right corner of i. Define a central region j. Let ctl_x and ctly denote the coordinates of the top-left corner of j and cbr_x and cbr_y denote the coordinates of the bottom-right corner of j. Then tl_x, tl_y, br_x, br_y, ctl_x, ctl_y, cbr_x and cbr_y should satisfy the following relationship:

![image-20220507183639302](C:\Users\72758\AppData\Roaming\Typora\typora-user-images\image-20220507183639302.png)

where n is odd and determines the scale of the central region j. In this paper, n is set to be 3 and 5 for the scales of bounding boxes less than and greater than 150, respectively. Figure 3 shows two central regions when n = 3 and n = 5, respectively. According to Equation (1), we can determine a scale-aware central region and then check whether the central region contains center keypoints

### 3.3. Enriching Center and Corner Information

  Center pooling. The geometric centers of objects do not always convey very recognizable visual patterns (e.g., the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). To address this issue, we propose center pooling to capture richer and more recognizable visual patterns. Figure 4(a) shows the principle of center pooling. The detailed process of center pooling is as follows: the backbone outputs a feature map and to determine whether a pixel in the feature map is a center keypoint, we need to find the maximum value in both the horizontal and vertical directions and add these values together. By doing so, center pooling helps improve the detection of center keypoints.

  Cascade corner pooling. Corners are often outside objects, which lack local appearance features. CornerNet [21] uses corner pooling to address this issue. The principle of corner pooling is shown in Figure 4(b). Corner pooling aims to find the maximum values on the boundary directions to determine corners. However, this makes corners sensitive to edges. To address this problem, we need to enable corners to extract features from central regions of the object. The principle of cascade corner pooling is presented in Figure 4(c). Cascade corner pooling first looks along a boundary to find a maximum boundary value and then looks inside the box along with the location of the boundary maximum value2 to find an internal maximum value; finally, the two maximum values are added together. By cascade corner pooling, the corners obtain both the boundary information and the visual patterns of objects.

  Both center pooling and the cascade corner pooling can be easily achieved by applying the corner pooling [21] in different directions. Figure 5(a) shows the structure of the center pooling module. To take a maximum value in a specific direction, e.g., the horizontal direction, we only need to connect the left pooling and the right pooling in sequence. Figure 5(b)shows the structure of a cascade top corner pooling module, in which the white rectangle denotes a 3 × 3 convolution followed by batch normalization. Compared with the top corner pooling in CornerNet [21], a left corner pooling is added before the top corner pooling.

### 3.4. Training and Inference

Training. Our method is implemented in Pytorch [31] and the network is trained from scratch. The resolution of the input image is 511 × 511, leading to heatmaps of the size 128×128. We use the data augmentation strategy presented in [21] to train a robust model. Adam [19] is used to optimize the training loss:

![image-20220507183959484](C:\Users\72758\AppData\Roaming\Typora\typora-user-images\image-20220507183959484.png)

where L co det and L ce det denote the focal losses, which are used to train the network to detect corners and center keypoints, respectively. L co pull is a “pull” loss for corners, which is used to minimize the distance of the embedding vectors that belongs to the same objects. L co push is a “push” loss for corners that is used to maximize the distance of the embedding vectors that belong to different objects. L co off and L ce off are ℓ1- losses [11], which are used to train the network to predict the offsets of corners and center keypoints, respectively. α, β and γ denote the weights for corresponding losses and are set to 0.1, 0.1 and 1, respectively. Ldet, Lpull, Lpush and Loff are all defined in CornerNet, and we suggest referring to [21] for details. We train CenterNet on 8 Tesla V100 (32GB) GPUs and use a batch size of 48. The maximum number of iterations is 480K. We use a learning rate of 2.5×10−4 for the first 450K iterations and then continue training 30K iterations with a rate of 2.5 × 10−5 . 

  Inference. Following [21], for the single-scale testing, we input both the original and horizontally flipped images with the original resolutions into the network. For multi-scale testing, we input both the original and horizontally flipped images with resolutions of 0.6, 1, 1.2, 1.5 and 1.8. We select top 70 center keypoints, top 70 top-left corners and top 70 bottom-right corners from the heatmaps to detect the bounding boxes. We flip the bounding boxes detected in the horizontally flipped images and mix them into the original bounding boxes. Soft-nms [2] is used to remove the redundant bounding boxes. We finally select the top 100 bounding boxes according to their scores as the final detection results.

